{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"NotebookAddons/DEVELOP_logo1.jpg\" width=\"250\" align=\"right\"/>\n",
    "\n",
    "# **Inundation Mapping from Time Series of Dual-Pol SAR Data**\n",
    "\n",
    "### **NASA Jet Propulsion Laboratory**\n",
    " \n",
    "### **DEVELOP Program Fall 2018 & Spring 2019 - Alaska Wetland Mapping**\n",
    "\n",
    "This Jupyter Notebook implements the SAR Wetland Extent Exploration Tool (SWEET), a thresholding-based approach to perform semi-automated mapping of wetland inundation.  Required inputs are intensity images of VV+VH polarization bands from Sentinel-1 C-SAR imagery.  Loaded images should already be co-registered, subset, radiometrically corrected, and in a projected coordinate system.  The notebook outputs classifications of typical, minimum, and maximum inundation states along with multi-looked classifications for individual dates.  The inundation classes are: open water, inundated, and not inundated. \n",
    "\n",
    "Additional information on the software requirements, Python dependencies, usage instructions, and project directory structure can be found in the README file.\n",
    "\n",
    "#### **Purpose:** \n",
    "\n",
    "Process Sentinel-1 VH/VV SAR data and produce maps showing wetland inundation extent in Alaska. Alaska is used as an example for this exercise. Application to other areas is possible. Note, however, that due to the limited penetration of Sentinel-1 C-band SAR data, application in densely vegetated areas may lead to an underestimation of inundation extent.\n",
    "\n",
    "<img style=\"padding: 7px\" src=\"NotebookAddons/website_Image2.png\" width=\"400\" align=\"right\"/>\n",
    "\n",
    "#### **Process:**\n",
    "- Read in data (assumes data is already subset and coregistered)\n",
    "- Select dates to include in analysis based on VV, VH, and VV/VH plots for each date and average backscatter timeseries plots (can exclude images that appear to have calibration errors)\n",
    "- Calculate multi-temporal averages for VV, VH, and VV/VH (Ia)\n",
    "- Classify Ia with thresholds to find typical inundation state\n",
    "- Perform multi-looking on individual dates and classify them to obtain classifications for each\n",
    "date\n",
    "- Determine maximum and minimum inundation state from classified individual dates\n",
    "- Export classified products as GeoTIFFs for validation \n",
    "\n",
    "#### **To do:**\n",
    "- Implement calibration correction using selected calibration sites\n",
    "- (Optional) Visual calibration checks:\n",
    "    1. Individual scene/multi-temporal average ratio plots\n",
    "    1. Average brightness in circle of variable radius around selected pixel over time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Background:**\n",
    "\n",
    "Synthetic Aperture Radar (SAR) polarimetric backscatter is a powerful datasource, with its ability to penetrate cloud cover and acquire imagery in low-light conditions.  Sentinel-1's C-band wavelength is particularly useful for wetland mapping, as the longer C-band wavelength can penetrate vegetation canopies to detect areas of inundated vegetation.\n",
    "\n",
    "Sentinel-1 has dual polarization, where it can transmit a signal in either horizontal (H) or vertical (V) polarizations and receive signals back both vertically and horizontally.  This notebook uses both vertically transmitted vertically recieved (VV) and vertically transmitted horizontally received (VH) intensity images and backscatter values.\n",
    "\n",
    "With SAR, pulses of microwave energy are emitted and received by the radar instrument's atenna.  When the pulses hit smooth surfaces such as open water, the microwave energy scatters and reflects away, resulting in a low return of backscatter to the radar.  Thus, flat surfaces like water appear as dark targets in the image.  In cases of inundated vegetation, the microwaves scatter off both the flat open water and the vertically emerging vegetation, resulting in a \"double-bounce\" reflection.  In these double-bounce reflections, a large portion of the transmitted waves are reflected back to the radar, resulting in brighter areas in the image.\n",
    "\n",
    "VV signals are of interest as they capture the increased brightness from the \"double bounce\" effect that commonly occurs with areas of inundated vegetation. This effect is even stronger in HH and at L-band wavelengths. Hence, future systems such as NISAR will improve the inundation mapping performance further by providing HH polarization data at L-band.\n",
    "\n",
    "To create an algorithm to identiy areas of inundation, we determined brightness threshold characteristics of inundated areas.  To identify shifts in inundation, we also looked at the changes over time in image brightness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import url_widget as url_w\n",
    "notebookUrl = url_w.URLWidget()\n",
    "display(notebookUrl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "from IPython.display import display\n",
    "\n",
    "notebookUrl = notebookUrl.value\n",
    "user = !echo $JUPYTERHUB_USER\n",
    "env = !echo $CONDA_PREFIX\n",
    "if env[0] == '':\n",
    "    env[0] = 'Python 3 (base)'\n",
    "if env[0] != '/home/jovyan/.local/envs/rtc_analysis':\n",
    "    display(Markdown(f'<text style=color:red><strong>WARNING:</strong></text>'))\n",
    "    display(Markdown(f'<text style=color:red>This notebook should be run using the \"rtc_analysis\" conda environment.</text>'))\n",
    "    display(Markdown(f'<text style=color:red>It is currently using the \"{env[0].split(\"/\")[-1]}\" environment.</text>'))\n",
    "    display(Markdown(f'<text style=color:red>Select the \"rtc_analysis\" from the \"Change Kernel\" submenu of the \"Kernel\" menu.</text>'))\n",
    "    display(Markdown(f'<text style=color:red>If the \"rtc_analysis\" environment is not present, use <a href=\"{notebookUrl.split(\"/user\")[0]}/user/{user[0]}/notebooks/conda_environments/Create_OSL_Conda_Environments.ipynb\"> Create_OSL_Conda_Environments.ipynb </a> to create it.</text>'))\n",
    "    display(Markdown(f'<text style=color:red>Note that you must restart your server after creating a new environment before it is usable by notebooks.</text>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### TEMPORARY FIX FOR MISSING EXTENSION\n",
    "print('This is a temporary fix. Check if you have \"jupyterlab-plotly\" extension installed:\\n')\n",
    "\n",
    "!jupyter labextension list\n",
    "\n",
    "res = input('\\nDid you have \"jupyterlab-plotly\" installed? (y/n)')\n",
    "\n",
    "if res.lower() != 'y' and res.lower() != 'yes':\n",
    "    print(\"You need to install them first. Then you will have to refresh this page: \")\n",
    "    print(\"Installing 'jupyterlab-plotly' extension...\")\n",
    "    !jupyter labextension install jupyterlab-plotly\n",
    "    \n",
    "    print('\\nNow refresh your browser.')\n",
    "    \n",
    "print(\"Note: This is a temp fix. You will encounter same issue when you restart your server\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check Python version:\n",
    "import sys\n",
    "pn = sys.version_info[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Import packages:\n",
    "# Packages for analysis:\n",
    "import rasterio #pip install rasterio (version >=1.0.8, requires GDAL >=2.3.1)\n",
    "%matplotlib inline\n",
    "import matplotlib #pip install matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np #pip install numpy\n",
    "import copy\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import subprocess, sys\n",
    "# Interactive widgets:\n",
    "import ipywidgets as widgets #pip install ipywidgets (included with Jupyter)\n",
    "from ipywidgets import Layout, VBox, Label, Checkbox, GridBox\n",
    "if pn == 2:\n",
    "    import cStringIO #needed for the image checkboxes\n",
    "elif pn == 3:\n",
    "    import io\n",
    "    import base64\n",
    "# Interactive time slider plots:\n",
    "import plotly #pip install plotly (version >= 3.0)\n",
    "import plotly.graph_objs as go\n",
    "from ipywidgets import interactive, HBox, VBox\n",
    "# Interactive time slider:\n",
    "import pandas as pd #pip install pandas\n",
    "## Interactive plots w/ pixel selection\n",
    "from mpldatacursor import datacursor\n",
    "# For exporting:\n",
    "from PIL import Image\n",
    "\n",
    "import opensarlab_lib as asfn\n",
    "asfn.jupytertheme_matplotlib_format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# All functions and classes and defined here in advance of their usage\n",
    "# below.  The original idea was to have this cell either hidden or \n",
    "# immutable for user's of this Notebook.\n",
    "#\n",
    "#### Define functions and classes:\n",
    "class DataFile: \n",
    "    ''' Data structure for extracting and storing relevant metadata about each\n",
    "    data file. Also contains methods for reading and closing data files.  For \n",
    "    now, assume all dates are in format YYYYMMDD and all file names follow the\n",
    "    same naming convention.\n",
    "    \n",
    "    Parameters:\n",
    "    - filepath: Path to the SAR image file being read\n",
    "    - convention: Tells the algorithm which naming convention to expect, and\n",
    "                consequently, how to extract metadata from the filename\n",
    "                Options: \"new\" - expects filename of the form:\n",
    "                               S1B_IW_RT10_20170423T040959_G_gpn_VV_subset.tif\n",
    "                         \"old\" - expects filename of the form:\n",
    "                               s1a-iw-rtch-vv-20170612T162820_coreg_subset.tif \n",
    "    Attributes:\n",
    "    - path: Path to the SAR image\n",
    "    - dir: Directory containing the SAR image\n",
    "    - name: Filename of SAR image\n",
    "    - ext: File extension of the SAR image (e.g .tif, .img)\n",
    "    - vtype: Polarization type: vv or vh\n",
    "    - date: Date string as extracted from filename (in form of yyyymmdd)\n",
    "    - datep: Date parsed into datetime format\n",
    "    - datef: Date formatted into a presentable string for labeling\n",
    "    - mean: Mean value of pixels in image\n",
    "    - min: Minimum value of pixels in image\n",
    "    - max: Maximum value of pixels in image\n",
    "    \n",
    "    Methods:\n",
    "    - read_file: Reads in SAR image file\n",
    "    - read_data: Reads in 2d array of raster values\n",
    "    - extract_metadata: Extracts date and polarization type from filename\n",
    "    - calculate_mean: Calculates mean pixel value\n",
    "    - calculate_min: Calculates minimum pixel value\n",
    "    - calculate_max: Calculates maximum pixel value\n",
    "    - close: Close SAR image file to reduce memory usage\n",
    "    - plot: Plots the image\n",
    "    '''\n",
    "    def __init__(self, filepath, convention=\"new\"):\n",
    "        self.path = filepath\n",
    "        self.dir = filepath.resolve().parent\n",
    "        self.name = str(filepath).split('/')[-1]\n",
    "        self.ext = str(filepath).split('.')[-1]\n",
    "        [self.vtype, self.date] = self.extract_metadata(self.name, convention)\n",
    "        self.datep = datetime.strptime(self.date, '%Y%m%d')\n",
    "        self.datef = self.datep.strftime(\"%B %d, %Y\")\n",
    "        self.mean = self.calculate_mean()\n",
    "        self.min = self.calculate_min()\n",
    "        self.max = self.calculate_max()\n",
    "        \n",
    "    def read_file(self):\n",
    "        '''Read in geospatial raster using rasterio'''\n",
    "        self.raw = rasterio.open(self.path)\n",
    "        return(self.raw)\n",
    "    \n",
    "    def read_data(self):\n",
    "        '''Read in 2d array of raster values'''\n",
    "        raw = self.read_file()\n",
    "        data = raw.read(1, out_shape=(1, int(raw.height), int(raw.width)))\n",
    "        return(data)\n",
    "    \n",
    "    def extract_metadata(self, filename, convention):\n",
    "        '''Extract metadata (vtype and date) from filename, assumes common \n",
    "        naming convention\n",
    "        '''\n",
    "        if convention == \"new\": #used for \n",
    "            if \"VV\" in filename: vtype = \"vv\"\n",
    "            elif \"VH\" in filename: vtype = \"vh\"\n",
    "            else:\n",
    "                vtype = \"Null\"\n",
    "                print(\"Could not detect vtype\")\n",
    "            date = filename.split('_')[3].split('T')[0]\n",
    "        elif convention == \"old\":\n",
    "            if \"-vv-\" in filename: vtype = \"vv\"\n",
    "            elif \"-vh-\" in filename: vtype = \"vh\"\n",
    "            else: \n",
    "                vtype = \"Null\"\n",
    "                print(\"Could not detect vtype\")\n",
    "            date = filename.split('-')[-1].split('T')[0]\n",
    "        return([vtype, date])\n",
    "    \n",
    "    def calculate_mean(self):\n",
    "        '''Calculate mean pixel value'''\n",
    "        mean = np.mean(self.read_data())\n",
    "        self.close()\n",
    "        return(mean)\n",
    "    \n",
    "    def calculate_min(self):\n",
    "        '''Calculate minimum pixel value'''\n",
    "        minval = np.min(self.read_data())\n",
    "        self.close()\n",
    "        return(minval)\n",
    "    \n",
    "    def calculate_max(self):\n",
    "        '''Calculate maximum pixel value'''\n",
    "        maxval = np.max(self.read_data())\n",
    "        self.close()\n",
    "        return(maxval)\n",
    "    \n",
    "    def close(self):\n",
    "        '''Close SAR image file to reduce memory usage'''\n",
    "        self.raw.close()\n",
    "        \n",
    "    def plot(self):\n",
    "        '''Plot image data contained in object'''\n",
    "        image_axis = plt.imshow(self.read_data(), cmap = plt.cm.gist_gray)\n",
    "        self.close()\n",
    "        return(image_axis)\n",
    "\n",
    "###############################################################################\n",
    "        \n",
    "class DateSet:\n",
    "    '''Data structure to hold original, multi-looked, and classified VV and VH \n",
    "    images and calculated VV/VH for each date.\n",
    "    \n",
    "    Parameters:\n",
    "    - date: Date string in form of yyyymmdd\n",
    "    - vv_df: VV DataFile corresponding to date\n",
    "    - vh_df: VH DataFile corresponding to date\n",
    "\n",
    "    Attributes:\n",
    "    - vv_df: VV DataFile corresponding to date\n",
    "    - vh_df: VH DataFile corresponding to date\n",
    "    - date: Date string in form of yyyymmdd\n",
    "    - datep: Date parsed into datetime format\n",
    "    - datef: Date formatted into a presentable string for labeling\n",
    "    - r_max: Maximum pixel value in VV/VH ratio\n",
    "    - r_min: Minimum pixel value in VV/VH ratio\n",
    "    - r_mean: Mean value of pixels in VV/VH ratio\n",
    "    - class_3x3: Classification product generated from multi-looked VV, VH, \n",
    "        and VV/VH values (appended to DateSet object after it is calculated)\n",
    "\n",
    "    Methods:\n",
    "    - close: Closes VV and VH image files\n",
    "    '''\n",
    "    def __init__(self, date, vv_df, vh_df):\n",
    "        if vv_df.date == vh_df.date:\n",
    "            self.vv_df = vv_df\n",
    "            self.vh_df = vh_df\n",
    "            self.date = date\n",
    "            self.datep = vv_df.datep\n",
    "            self.datef = vh_df.datef\n",
    "            r = self.vv_df.read_data()/self.vh_df.read_data()\n",
    "            self.r_min = np.min(r)\n",
    "            self.r_max = np.max(r)\n",
    "            self.r_mean = np.mean(r)\n",
    "            self.close()\n",
    "            self.class_3x3 = None\n",
    "        else:\n",
    "            print(\"Could not create DateSet object. VV and VH dates do not match\")\n",
    "    def close(self):\n",
    "        '''Close VV and VH image files by calling close method in VV and VH\n",
    "        DataFile objects\n",
    "        '''\n",
    "        self.vv_df.close()\n",
    "        self.vh_df.close()\n",
    "    \n",
    "###############################################################################\n",
    "\n",
    "def gray_plot(image, vmin=0, vmax=2, return_ax=False):\n",
    "    '''Plots an image in grayscale.\n",
    "    \n",
    "    Parameters:\n",
    "    - image: 2D array of raster values\n",
    "    - vmin: Minimum value for colormap\n",
    "    - vmax: Maximum value for colormap\n",
    "    - return_ax: Option to return plot axis\n",
    "\n",
    "    '''\n",
    "    ax = plt.imshow(image, cmap = plt.cm.gist_gray)\n",
    "    plt.clim(vmin,vmax)\n",
    "    if return_ax:\n",
    "        return(ax)\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "def pixel2pixel_plot(image, vmin=0, vmax=.2, dpi=192, cursor=True, full=True):\n",
    "    '''Plots a full resolution image of a map where each pixel in the map\n",
    "    corresponds to a pixel in the user's monitor.\n",
    "    \n",
    "    Parameters:\n",
    "    - image: 2d array of raster values\n",
    "    - vmin: Minimum value for colormap\n",
    "    - vmax: Maximum value for colormap\n",
    "    - dpi: User monitor DPI\n",
    "    - cursor: Option to include an interactive datacursor\n",
    "    - full: Option to increase the width of the Jupyter Notebook to the full\n",
    "        size of the user's monitor.\n",
    "    '''\n",
    "    if full:\n",
    "        from IPython.core.display import display, HTML\n",
    "        display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "    xsize = np.size(image,1)\n",
    "    ysize = np.size(image,0)\n",
    "    plt.figure(figsize=(xsize/dpi, ysize/dpi),dpi=dpi)\n",
    "    ax = gray_plot(image, vmin, vmax, True)\n",
    "    if cursor:\n",
    "        dc = datacursor(ax)\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "###############################################################################\n",
    "\n",
    "    \n",
    "def big_fig(x=20,y=10):\n",
    "    '''Initializes a large figure.\n",
    "    \n",
    "    Parameters:\n",
    "    - x, y: X and Y figure dimensions\n",
    "    '''\n",
    "    return(plt.figure(figsize=(x,y)))\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "def get_files(data_directory, ext=\"tif\", search=\"\", convention=\"new\"):\n",
    "    '''Returns a list of the files in the data directory that have the desired\n",
    "    extension.  \n",
    "    \n",
    "    Parameters:\n",
    "    - data_directory: Directory containing data files\n",
    "    - ext: Desired file extension (default is .tif)\n",
    "    - search: String that the function searchs for in filenames (default is \"\")\n",
    "    - convention: Naming convention type expected by DataFile (\"old\" or \"new\")\n",
    "    '''\n",
    "    file_list = []\n",
    "    ext_len = len(ext)\n",
    "\n",
    "    for filename in data_directory.iterdir():\n",
    "        filename_str = str(filename)\n",
    "        if filename_str[-ext_len::] == ext:\n",
    "            if search not in filename_str: continue\n",
    "            filepath = data_directory/filename_str\n",
    "            data_file = DataFile(filepath, convention)\n",
    "            file_list.append(data_file)\n",
    "    return(file_list)\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "def find_minmax(date_sets):\n",
    "    '''Returns a list of nested lists containing minimum and maximum VV, VH, \n",
    "    and VV/VH values found from a list of DateSets to standardize color \n",
    "    limits when plotting.\n",
    "    \n",
    "    Parameters:\n",
    "    - date_sets: List of DateSets to compare\n",
    "    '''\n",
    "    vv_running_min = 10\n",
    "    vv_running_max = 0\n",
    "    vh_running_min = 10\n",
    "    vh_running_max = 0\n",
    "    r_running_min = 10\n",
    "    r_running_max = 0\n",
    "    for ds in date_sets:\n",
    "        # Check VV values\n",
    "        if ds.vv_df.min < vv_running_min: vv_running_min = ds.vv_df.min\n",
    "        if ds.vv_df.max > vv_running_max: vv_running_max = ds.vv_df.max\n",
    "        # Check VH values\n",
    "        if ds.vh_df.min < vh_running_min: vh_running_min = ds.vh_df.min\n",
    "        if ds.vh_df.max > vh_running_max: vh_running_max = ds.vh_df.max\n",
    "        # Check VV/VH values\n",
    "        if ds.r_min < r_running_min: r_running_min = ds.r_min\n",
    "        if ds.r_max > r_running_max: r_running_max = ds.r_max\n",
    "    print(\"VV value range:\")\n",
    "    print(vv_running_min, vv_running_max)\n",
    "    print(\"VH value range:\")\n",
    "    print(vh_running_min, vh_running_max)\n",
    "    print(\"VV/VH value range:\")\n",
    "    print(r_running_min, r_running_max)\n",
    "    return([[vv_running_min, vv_running_max], \n",
    "            [vh_running_min, vh_running_max], \n",
    "            [r_running_min, r_running_max]\n",
    "           ]\n",
    "          )\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "def find_dates(data_files):\n",
    "    '''Returns a list of unique dates in a list of VV and VH DataFiles.\n",
    "    \n",
    "    Parameters:\n",
    "    - data_files: List of DataFiles to compare\n",
    "    '''\n",
    "    date_set = set()\n",
    "    for df in data_files:\n",
    "        date_set.add(df.date)\n",
    "    date_list = np.sort(list(date_set)) #convert set to list to access by index\n",
    "    return(date_list)\n",
    "###############################################################################\n",
    "\n",
    "def make_datesets(data_files, date_list):\n",
    "    '''Returns a list of DateSet objects from list of DataFile objects and\n",
    "    corresponding dates.\n",
    "    \n",
    "    Parameters:\n",
    "    - data_files: List of DataFiles to combine into DateSets\n",
    "    - date_list: List of corresponding dates\n",
    "    '''\n",
    "    date_dic = {}\n",
    "    dateset_list = []\n",
    "    for date in date_list:\n",
    "        vtype_dic = {}\n",
    "        date_dic[date] = vtype_dic\n",
    "    for df in data_files:\n",
    "        date = df.date\n",
    "        vtype = df.vtype\n",
    "        date_dic[date][vtype] = df\n",
    "    for date in date_list:\n",
    "        vtype_dic = date_dic[date]\n",
    "        dateset = DateSet(date, vtype_dic['vv'], vtype_dic['vh'])\n",
    "        dateset_list.append(dateset)\n",
    "    return(dateset_list)\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "def vv_vh_r_plot(vv, vh, r, \n",
    "                 vv_min, vv_max, \n",
    "                 vh_min, vh_max, \n",
    "                 r_min=0, r_max=6, \n",
    "                 axstr=\"\",\n",
    "                 xsize=9.5, ysize=3\n",
    "                ):\n",
    "    ''' Plots a 3 panel figure of VV, VH, and VV/VH for a given date.\n",
    "    \n",
    "    Parameters:\n",
    "    - vv, vh, r: 2D arrays of VV, VH, and R values\n",
    "    - vv_min, vv_max: Minimum and maximum VV values for colormap limits\n",
    "    - vh_min, vh_max: Minimum and maximum VH values for colormap limits\n",
    "    - r_min, r_max: Minimum and maximum VV/VH values for colormap limits\n",
    "    - axstr: String to plot along right side of figure (default is \"\")\n",
    "    - xsize, ysize: X and Y dimensions for figure\n",
    "    '''\n",
    "    fig = big_fig(xsize, ysize)\n",
    "    # VV plot\n",
    "    ax = plt.subplot(1, 3, 1)\n",
    "    gray_plot(vv, vv_min, vv_max)\n",
    "    plt.title(\"VV\")\n",
    "    # VH plot\n",
    "    ax = plt.subplot(1, 3, 2)\n",
    "    gray_plot(vh, vh_min, vh_max)\n",
    "    plt.title(\"VH\")\n",
    "    # VV/VH plot\n",
    "    ax = plt.subplot(1, 3, 3)\n",
    "    gray_plot(r, r_min, r_max)\n",
    "    plt.title(\"VV/VH\")\n",
    "    if axstr:\n",
    "        # Set axis string\n",
    "        ax.text(1.05, 0.5, axstr, rotation=90, ha='left',\n",
    "                va='center', transform=ax.transAxes\n",
    "               )\n",
    "    return(fig)\n",
    "###############################################################################\n",
    "\n",
    "def vv_vh_cov_plot(vv_cov, vh_cov, \n",
    "                 vv_min, vv_max, \n",
    "                 vh_min, vh_max, \n",
    "                 axstr=\"\",\n",
    "                 xsize=9.5, ysize=3\n",
    "                ):\n",
    "    ''' Plots a 3 panel figure of VV, VH, and VV/VH for a given date.\n",
    "    \n",
    "    Parameters:\n",
    "    - vv, vh, r: 2D arrays of VV, VH, and R values\n",
    "    - vv_min, vv_max: Minimum and maximum VV values for colormap limits\n",
    "    - vh_min, vh_max: Minimum and maximum VH values for colormap limits\n",
    "    - r_min, r_max: Minimum and maximum VV/VH values for colormap limits\n",
    "    - axstr: String to plot along right side of figure (default is \"\")\n",
    "    - xsize, ysize: X and Y dimensions for figure\n",
    "    '''\n",
    "    fig = big_fig(xsize, ysize)\n",
    "    # VV cov plot\n",
    "    ax = plt.subplot(1, 2, 1)\n",
    "    gray_plot(vv_cov, vv_min, vv_max)\n",
    "    plt.title(\"VV Coefficient of Variation\")\n",
    "    # VH cov plot\n",
    "    ax = plt.subplot(1, 2, 2)\n",
    "    gray_plot(vh_cov, vh_min, vh_max)\n",
    "    plt.title(\"VH Coefficient of Variation\")\n",
    "    if axstr:\n",
    "        # Set axis string\n",
    "        ax.text(1.05, 0.5, axstr, rotation=90, ha='left',\n",
    "                va='center', transform=ax.transAxes\n",
    "               )\n",
    "    return(fig)\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "def save_plot3(date_set, \n",
    "               vv_min, vv_max, \n",
    "               vh_min, vh_max, \n",
    "               r_min=0, r_max=6,\n",
    "               pn=pn\n",
    "              ):\n",
    "    '''Returns a 3 panel (VV, VH, and VV/VH) plot saved using cStringIO so it\n",
    "    can be used in multi_checkbox_widget_dateimages.\n",
    "    \n",
    "    Parameters:\n",
    "    - date_set: DateSet object\n",
    "    - vv_min, vv_max: Minimum and maximum VV values for colormap limits\n",
    "    - vh_min, vh_max: Minimum and maximum VH values for colormap limits\n",
    "    - r_min, r_max: Minimum and maximum VV/VH values for colormap limits\n",
    "    - pn: Python version number (2 or 3)\n",
    "    '''\n",
    "    plt.ioff()\n",
    "    vv = date_set.vv_df.read_data()\n",
    "    vh = date_set.vh_df.read_data()\n",
    "    r = vv/vh\n",
    "    axstr = date_set.datef\n",
    "    fig = vv_vh_r_plot(vv, vh, r, \n",
    "                       vv_min, vv_max, \n",
    "                       vh_min, vh_max, \n",
    "                       r_min, r_max, \n",
    "                       axstr\n",
    "                      )\n",
    "    date_set.close()\n",
    "    if pn == 2:\n",
    "        figdata = cStringIO.StringIO()\n",
    "        fig.savefig(figdata, format='png')\n",
    "        plt.close(fig)\n",
    "        plt.ion()\n",
    "        return(figdata)\n",
    "    elif pn == 3:\n",
    "        figdata = io.BytesIO()\n",
    "        fig.savefig(figdata, format='png')\n",
    "        plt.close(fig)\n",
    "        plt.ion()\n",
    "        return(base64.b64encode(figdata.getvalue()).decode())\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "def multi_checkbox_widget_dateimages(date_sets, \n",
    "                                     vv_min, vv_max, \n",
    "                                     vh_min, vh_max, \n",
    "                                     r_min=0, r_max=6,\n",
    "                                     pn=pn\n",
    "                                    ):\n",
    "    '''Returns a table of 3-paneled (VV, VH, and VV/VH) plots for each date\n",
    "    with checkboxes next to each row so user can select dates to exclude based\n",
    "    on visual inspection.\n",
    "    \n",
    "    Parameters:\n",
    "    - date_sets: List of DateSet objects\n",
    "    - vv_min, vv_max: Minimum and maximum VV values for colormap limits\n",
    "    - vh_min, vh_max: Minimum and maximum VH values for colormap limits\n",
    "    - r_min, r_max: Minimum and maximum VV/VH values for colormap limits\n",
    "    - pn: Python version number (2 or 3)\n",
    "    '''\n",
    "    nds = len(date_sets)\n",
    "    descriptions = [save_plot3(ds, vv_min, vv_max, vh_min, vh_max, r_min, r_max) \n",
    "                    for ds in date_sets\n",
    "                   ]\n",
    "    item_layout = Layout(height='auto', min_width='1000px')\n",
    "    if pn == 2:\n",
    "        items = [Checkbox(layout=item_layout, \n",
    "                          description='<img src=\\\"data:/png;base64,%s\\\"/>' % desc\n",
    "                          .getvalue()\n",
    "                          .encode(\"base64\")\n",
    "                          .strip(), \n",
    "                          value=False\n",
    "                         ) for desc in descriptions\n",
    "                ]\n",
    "    elif pn == 3:\n",
    "        items = [Checkbox(layout=item_layout,\n",
    "                         description='<img src=\\\"data:/png;base64,%s\\\"/>' % desc\n",
    "                         .strip(),\n",
    "                         value=False\n",
    "                         ) for desc in descriptions\n",
    "                ]\n",
    "    box_layout = Layout(border='3px solid black',\n",
    "                        width='auto',\n",
    "                        height='',\n",
    "                       grid_template_columns='1',\n",
    "                       grid_template_rows='auto'\n",
    "                       )\n",
    "    grid = GridBox(children=items, layout=box_layout)\n",
    "    ws = VBox([Label('Select images to exclude:'), grid])\n",
    "    return(ws)\n",
    "\n",
    "###############################################################################\n",
    "def get_excluded_dates3(ws, date_sets):\n",
    "    '''Returns a list of dates that were selected by the user for exclusion\n",
    "    in the image checkbox table.\n",
    "    \n",
    "    Parameters:\n",
    "    - ws: Handle of image checkbox with selections\n",
    "    - date_sets: List of DateSets used as input to the image checkbox \n",
    "    '''\n",
    "    excluded_idx = [idx \n",
    "                    for idx, ws \n",
    "                    in enumerate(ws.children[1].children) \n",
    "                    if ws.value\n",
    "                   ]\n",
    "    excluded_dates = [date_sets[idx].datef for idx in excluded_idx]\n",
    "    return(excluded_dates)\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "def interactive_backscatter_plot(date_sets):\n",
    "    '''Creates an interactive timeseries plot of backscatter values for\n",
    "    a site.  Modified code from https://plot.ly/python/range-slider/\n",
    "    \n",
    "    Parameters:\n",
    "    - date_sets: List of DateSets\n",
    "    '''\n",
    "    dateps = []\n",
    "    vv_means = []\n",
    "    vh_means = []\n",
    "    for ds in date_sets:\n",
    "        dateps.append(ds.datep)\n",
    "        vv_means.append(10*np.log10(ds.vv_df.mean))\n",
    "        vh_means.append(10*np.log10(ds.vh_df.mean))\n",
    "\n",
    "    vv_trace = go.Scatter(\n",
    "      x = dateps,\n",
    "      y = vv_means,\n",
    "      name = \"vv\",\n",
    "      text = [str(vvm) for vvm in vv_means],\n",
    "      yaxis = \"y\"\n",
    "    )\n",
    "\n",
    "    vh_trace = go.Scatter(\n",
    "      x = dateps,\n",
    "      y = vh_means,\n",
    "      name = \"vh\",\n",
    "      text = [str(vhm) for vhm in vh_means],\n",
    "      yaxis = \"y\"\n",
    "    )\n",
    "\n",
    "    data = [vv_trace, vh_trace]\n",
    "\n",
    "    # style all the traces\n",
    "    for k in range(len(data)):\n",
    "        data[k].update(\n",
    "            {\n",
    "                \"hoverinfo\": \"name+x+text\",\n",
    "                \"line\": {\"width\": 0.5}, \n",
    "                \"marker\": {\"size\": 8},\n",
    "                \"mode\": \"lines+markers\",\n",
    "                \"showlegend\": False\n",
    "            }\n",
    "        )\n",
    "\n",
    "    layout = {\n",
    "      \"dragmode\": \"zoom\", \n",
    "      \"hovermode\": \"x\", \n",
    "      \"legend\": {\"traceorder\": \"reversed\"}, \n",
    "      \"margin\": {\n",
    "        \"t\": 100, \n",
    "        \"b\": 100\n",
    "      }, \n",
    "      \"xaxis\": {\n",
    "        \"autorange\": True, \n",
    "        \"range\": [\"2017-10-31 18:36:37.3129\", \"2018-05-10 05:23:22.6871\"], \n",
    "        \"rangeslider\": {\n",
    "          \"autorange\": True, \n",
    "          \"range\": [\"2017-10-31 18:36:37.3129\", \"2018-05-10 05:23:22.6871\"]\n",
    "        }, \n",
    "        \"type\": \"date\"\n",
    "      }, \n",
    "      \"yaxis\": {\n",
    "        \"anchor\": \"x\", \n",
    "        \"autorange\": True, \n",
    "        \"domain\": [0, 1], \n",
    "        \"linecolor\": \"#673ab7\", \n",
    "        \"mirror\": True, \n",
    "        \"range\": [-20,20], \n",
    "        \"showline\": True, \n",
    "        \"side\": \"right\", \n",
    "        \"tickfont\": {\"color\": \"#673ab7\"}, \n",
    "        \"tickmode\": \"auto\", \n",
    "        \"ticks\": \"\",\n",
    "        \"title\": \"Power (dB)\",\n",
    "        \"titlefont\": {\"color\": \"#673ab7\"}, \n",
    "        \"type\": \"linear\", \n",
    "        \"zeroline\": False\n",
    "      }\n",
    "    }\n",
    "    plotly.offline.init_notebook_mode(connected=True)\n",
    "    fig = go.Figure(data=data, layout=layout)\n",
    "    plotly.offline.iplot(fig)\n",
    "    \n",
    "###############################################################################\n",
    "\n",
    "def make_timeslider(date_sets):\n",
    "    '''Creates and returns the handle to an interactive time slider\n",
    "    that the user can use to select a time range of values to include.\n",
    "    \n",
    "    Parameters: \n",
    "    - date_sets: List of DateSets\n",
    "    '''\n",
    "    start_date = date_sets[0].datef\n",
    "    end_date = date_sets[-1].datef\n",
    "    date_range = pd.date_range(start_date, end_date, freq='D')\n",
    "    options = [(date.strftime(' %b %d, %Y '), date) for date in date_range]\n",
    "    index = (0, len(options)-1)\n",
    "    selection_range_slider = widgets.SelectionRangeSlider(\n",
    "    options=options,\n",
    "    index=index,\n",
    "    description='Dates',\n",
    "    orientation='horizontal',\n",
    "    layout={'width': '500px'})\n",
    "    return(selection_range_slider)  \n",
    "\n",
    "###############################################################################\n",
    "\n",
    "def get_slider_vals(selection_range_slider):\n",
    "    '''Returns the minimum and maximum dates retrieved from the\n",
    "    interactive time slider.\n",
    "    \n",
    "    Parameters:\n",
    "    - selection_range_slider: Handle of the interactive time slider\n",
    "    '''\n",
    "    [a,b] = list(selection_range_slider.value)\n",
    "    slider_min = a.to_pydatetime()\n",
    "    slider_max = b.to_pydatetime()\n",
    "    return(slider_min, slider_max)\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "def filter_date_sets(date_sets, excluded_dates, slider_min, slider_max):\n",
    "    '''Returns a list of DateSets that are filtered according to the\n",
    "    excluded dates from the checkbox table and start/end dates retrieved from\n",
    "    the interactive time slider.\n",
    "    \n",
    "    Parameters:\n",
    "    - date_sets: List of DateSets to be filtered\n",
    "    - excluded_dates: Dates chosen for exclusion from checkbox table\n",
    "    - slider_min, slider_max: Minimum and maximum date values from time slider\n",
    "    '''\n",
    "    filtered_date_sets = []\n",
    "    for ds in date_sets:\n",
    "        in_time_range = ds.datep >= slider_min and ds.datep <= slider_max\n",
    "        if ds.datef not in excluded_dates and in_time_range:\n",
    "            filtered_date_sets.append(ds)\n",
    "    return(filtered_date_sets)\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "def plot_all_dates(date_sets, \n",
    "                   vv_min, vv_max, \n",
    "                   vh_min, vh_max, \n",
    "                   r_min=0, r_max=6, \n",
    "                   xsize=10, ysize=20):\n",
    "    '''Plots  3-panel images (VV, VH, and VV/VH) for each date.\n",
    "    \n",
    "    Parameters:\n",
    "    - date_sets: List of DateSets\n",
    "    - vv_min, vv_max: Minimum and maximum VV values for colormap limits\n",
    "    - vh_min, vh_max: Minimum and maximum VH values for colormap limits\n",
    "    - r_min, vr_max: Minimum and maximum VV/VH values for colormap limits\n",
    "    - xsize, ysize: X and Y figure dimensions\n",
    "    '''\n",
    "    big_fig(xsize,ysize) # initialize a large figure\n",
    "    nds = len(date_sets)\n",
    "    for ii in range(0, nds):\n",
    "        pos = 3*ii + 1\n",
    "        # VV plot\n",
    "        ax = plt.subplot(nds, 3, pos)\n",
    "        gray_plot(date_sets[ii].vv_df.read_data(), vv_min, vv_max)\n",
    "        if ii == 0: plt.title(\"VV\")\n",
    "        # VH plot\n",
    "        ax = plt.subplot(nds, 3, pos+1)\n",
    "        gray_plot(date_sets[ii].vh_df.read_data(), vh_min, vh_max)\n",
    "        if ii == 0: plt.title(\"VH\")\n",
    "        # VV/VH plot\n",
    "        ax = plt.subplot(nds, 3, pos+2)\n",
    "        gray_plot(\n",
    "            date_sets[ii].vv_df.read_data()/date_sets[ii].vh_df.read_data(), \n",
    "            r_min, r_max\n",
    "                 )\n",
    "        ax.text(1.05, 0.5, date_sets[ii].datef, \n",
    "                rotation=90, \n",
    "                ha='left', \n",
    "                va='center', \n",
    "                transform=ax.transAxes\n",
    "               )\n",
    "        if ii == 0: plt.title(\"VV/VH\")\n",
    "        date_sets[ii].close()\n",
    "        \n",
    "###############################################################################\n",
    "\n",
    "def calculate_temporal_avg(data_file_list, \n",
    "                           vtype=\"vh\", \n",
    "                           start_date=0, \n",
    "                           end_date=9*10**13, \n",
    "                           verbose=False\n",
    "                          ):\n",
    "    '''Calculates and returns the multi-temporal average of VV or VH data \n",
    "    using a running sum.\n",
    "    \n",
    "    Parameters:\n",
    "    - data_file_list: List of DataFiles.\n",
    "    - vtype: SAR polarization type (vv or vh)\n",
    "    - start_date, end_date: Start and end dates as numeric values\n",
    "    - verbose: Option to print a message each time a DataFile is not of the\n",
    "        correct vtype, or its date does not fall in the time range between\n",
    "        start_date and end_date\n",
    "    '''\n",
    "    files_in_range = []\n",
    "    count = 0\n",
    "    for data_file in data_file_list:\n",
    "        check = True\n",
    "        while check:\n",
    "            if data_file.vtype is not vtype:\n",
    "                if verbose: print(\n",
    "                    \"Data is not of the correct polarization(VV or VH)\"\n",
    "                                 )\n",
    "                break\n",
    "            file_date = int(data_file.date)\n",
    "            if file_date < start_date or file_date > end_date:\n",
    "                if verbose: print(\n",
    "                    \"Data is not within the specified time frame\"\n",
    "                )\n",
    "                break\n",
    "            data = data_file.read_data()\n",
    "            if count == 0:\n",
    "                running_sum = data\n",
    "                running_sum2 = data**2\n",
    "            else:\n",
    "                if data.shape != data.shape:\n",
    "                    if verbose: print(\n",
    "                        \"Data are not the same dimensions (row x colum)\"\n",
    "                    )\n",
    "                    break\n",
    "                running_sum += data\n",
    "                running_sum2 += data**2\n",
    "            count += 1\n",
    "            data_file.close()\n",
    "            files_in_range.append(data_file)\n",
    "            check = False\n",
    "    temporal_mean = running_sum/count\n",
    "    st_dev = np.sqrt((running_sum2 - (running_sum**2)/count)/(count-1))\n",
    "    coef_of_variation = st_dev/temporal_mean\n",
    "    return([temporal_mean, files_in_range, coef_of_variation])\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "def calculate_r_cov(date_sets):\n",
    "    '''Calculate the coefficient of variation for the VV/VH ratio.\n",
    "    \n",
    "    Parameters:\n",
    "    - date_sets: List of DateSets\n",
    "    '''\n",
    "    count = 0\n",
    "    for date in date_sets:\n",
    "        vv = date.vv_df.read_data()\n",
    "        vh = date.vh_df.read_data()\n",
    "        r = vv/vh\n",
    "        date.close()\n",
    "        if count == 0:\n",
    "            running_sum = r\n",
    "            running_sum2 = r**2\n",
    "        else:\n",
    "            running_sum += r\n",
    "            running_sum2 += r**2\n",
    "        count += 1\n",
    "    temporal_mean = running_sum/count\n",
    "    st_dev = np.sqrt((running_sum2 - (running_sum**2)/count)/(count-1))\n",
    "    r_cov = st_dev/temporal_mean\n",
    "    return(r_cov)\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "def calculate_multitemporal_avg(date_sets):\n",
    "    '''Calculates and returns the multi-temporal VV, VH, and VV/VH averages\n",
    "    from a list of DateSets.\n",
    "    \n",
    "    Parameters:\n",
    "    - date_sets: List of DateSets\n",
    "    '''\n",
    "    vv_data_files = [ds.vv_df for ds in date_sets]\n",
    "    vh_data_files = [ds.vh_df for ds in date_sets]\n",
    "    [vv_avg, vv_files, vv_cov] = calculate_temporal_avg(vv_data_files, \"vv\")\n",
    "    [vh_avg, vh_files, vh_cov] = calculate_temporal_avg(vh_data_files, \"vh\")\n",
    "    r_avg = vv_avg/vh_avg\n",
    "    return(vv_avg, vh_avg, r_avg, vv_cov, vh_cov)\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "class ClassThresholds:\n",
    "    '''Class that contains upper and lower bounds for VV, VH, and VV/VH for\n",
    "    a class as well as a method for checking membership based on VV, VH, and \n",
    "    VV/VH values.\n",
    "    \n",
    "    Parameters:\n",
    "    - vv_low, vv_hi: Lower and upper bounds for VV values in the class\n",
    "    - vh_low, vh_hi: Lower and upper bounds for VH values in the class\n",
    "    - r_low, r_hi: Lower and upper bounds for VV/VH values in the class\n",
    "    \n",
    "    Methods:\n",
    "    - is_member: Check a set of VV, VH, and VV/VH values from a pixel to see\n",
    "        if that pixel should be included in this class\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 vv_low=-9999, vv_hi=9999, \n",
    "                 vh_low=-9999, vh_hi=9999, \n",
    "                 r_low=-9999, r_hi=9999):\n",
    "        self.vv_low = vv_low\n",
    "        self.vv_hi = vv_hi\n",
    "        self.vh_low = vh_low\n",
    "        self.vh_hi = vh_hi\n",
    "        self.r_low = r_low\n",
    "        self.r_hi = r_hi\n",
    "    \n",
    "    def is_member(self, vv_val, vh_val, r_val):\n",
    "        '''Checks if the input VV, VH, and VV/VH values are sufficient\n",
    "        to classify the pixel in this class.\n",
    "        '''\n",
    "        vv_cond = vv_val > self.vv_low and vv_val < self.vv_hi\n",
    "        vh_cond = vh_val > self.vh_low and vh_val < self.vh_hi\n",
    "        r_cond = r_val > self.r_low and r_val < self.r_hi\n",
    "        result = vv_cond and vh_cond and r_cond\n",
    "        return(result)\n",
    "    \n",
    "###############################################################################\n",
    "\n",
    "def pixel_radius_average(image, mid_row, mid_col, radius):\n",
    "    '''Calculates and returns the average pixel value for an image in a \n",
    "    variable radius around a selected pixel.\n",
    "    \n",
    "    Parameters:\n",
    "    - image: 2D array of raster values\n",
    "    - mid_row, mid_col: Row and column coordinates of the selected pixel\n",
    "    - radius: Radius size of the moving window (in pixels)\n",
    "    '''\n",
    "    # Find indices of pixels within the radius\n",
    "    row_rng = range(mid_row-radius,mid_row+radius+1)\n",
    "    col_rng = range(mid_col-radius,mid_col+radius+1)\n",
    "    good_inds = []\n",
    "    for row in row_rng:\n",
    "        row_ind = row\n",
    "        y_val = row - mid_row\n",
    "        for col in col_rng:\n",
    "            col_ind = col\n",
    "            x_val = col - mid_col\n",
    "            dist = x_val^2 + y_val^2\n",
    "            if dist <= radius^2:\n",
    "                good_inds.append([row, col])\n",
    "    # Get values of pixels in radius\n",
    "    vals_in_radius = []\n",
    "    for pix in good_inds:\n",
    "        row = pix[0]\n",
    "        col = pix[1]\n",
    "        try:\n",
    "            val = image[row][col]\n",
    "            vals_in_radius.append(val)\n",
    "        except Exception as e:\n",
    "            print(\"Could not add pixel to list; check edge effects\")\n",
    "            print(e)\n",
    "    # Calculate average\n",
    "    mean = np.mean(vals_in_radius)\n",
    "    return(mean)\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "def moving_window(data, radius=3, verbose=False):\n",
    "    '''Applies a moving window average to a 2D array and returns the result.\n",
    "    \n",
    "    Parameters:\n",
    "    - data: 2D array of raster values\n",
    "    - radius: Radius size in pixels\n",
    "    - verbose: Option to print a message whenever a pixel cannot be added to\n",
    "        the window\n",
    "    '''\n",
    "    [nrow, ncol] = data.shape\n",
    "    windowed = copy.copy(data)\n",
    "    for row in range(nrow):\n",
    "        for col in range(ncol):\n",
    "            vals_in_window = []\n",
    "            row_rng = range(row-radius,row+radius+1)\n",
    "            col_rng = range(col-radius,col+radius+1)\n",
    "            for y in row_rng:\n",
    "                for x in col_rng:\n",
    "                    try:\n",
    "                        val = data[y][x]\n",
    "                        vals_in_window.append(val)\n",
    "                    except Exception as e:\n",
    "                        if verbose:\n",
    "                            print(\"Could not add pixel to list, \" +\n",
    "                                  \"may be due to edge effects\")\n",
    "                            print(e)\n",
    "            windowed[row][col] = np.mean(vals_in_window)\n",
    "    return(windowed)\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "def triple_classify(vv_data, vh_data, r_data,\n",
    "                    water_thresh, flooded_thresh\n",
    "                   ):\n",
    "    '''Makes and returns a classification product based on VV, VH, and VV/VH\n",
    "    values.\n",
    "    \n",
    "    Parameters:\n",
    "    - vv_data, vh_data, r_data: VV, VH, and VV/VH arrays\n",
    "    - water_thresh: ClassThreshold for the open water class\n",
    "    - flooded_thresh: ClassThreshold for the inundated class\n",
    "    '''\n",
    "    classified_image = copy.copy(r_data)\n",
    "    for row in range(classified_image.shape[0]):\n",
    "        for col in range(classified_image.shape[1]):\n",
    "            vv_val = float(vv_data[row][col])\n",
    "            vh_val = float(vh_data[row][col])\n",
    "            r_val = float(r_data[row][col])\n",
    "            water = water_thresh.is_member(vv_val, vh_val, r_val)\n",
    "            flooded = flooded_thresh.is_member(vv_val, vh_val, r_val)\n",
    "            if water:\n",
    "                classified_image[row][col] = 1\n",
    "            elif flooded:\n",
    "                classified_image[row][col] = 3\n",
    "            else:\n",
    "                classified_image[row][col] = 2\n",
    "    return(classified_image)\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "def plot_class_cov(typical_inundation, vv_cov, vh_cov,\n",
    "                  xsize=10,ysize=5): \n",
    "    '''Makes and returns a classification product based on VV, VH, and VV/VH\n",
    "    values.\n",
    "    \n",
    "    Parameters:\n",
    "    - typical_inundation: \n",
    "    - vv_cov, vh_cov: VV and VH coefficient of variance (cov) distributions\\\n",
    "    - xsize, ysize: size parameters for the figure\n",
    "    '''\n",
    "    kwargs = dict(histtype='stepfilled', alpha=.8, bins=1000)\n",
    "    nrows = 2\n",
    "    ncols = 3\n",
    "    npanels = nrows*ncols\n",
    "    big_fig(xsize, ysize)\n",
    "    for x in range(1,npanels+1):\n",
    "        if x < 4:\n",
    "            vtype = \"VV\"\n",
    "            vals = vv_cov\n",
    "            class_num = x\n",
    "        else:\n",
    "            vtype = \"VH\"\n",
    "            vals = vh_cov\n",
    "            class_num = x - 3\n",
    "        indx = np.where(typical_inundation == class_num)\n",
    "        ax = plt.subplot(nrows,ncols,x)\n",
    "        plt.hist(vals[indx], **kwargs)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "        if x < 4:\n",
    "            plt.title(\"Class: \" + str(x))\n",
    "        if x == 3:\n",
    "            ax.text(1.05, 0.5, \"VV Coefficient of Variation\", rotation=90, ha='left',\n",
    "                va='center', transform=ax.transAxes\n",
    "               )\n",
    "        if x == 6:\n",
    "            ax.text(1.05, 0.5, \"VH Coefficient of Variation\", rotation=90, ha='left',\n",
    "                va='center', transform=ax.transAxes\n",
    "               )\n",
    "        \n",
    "###############################################################################\n",
    "\n",
    "def refined_classify(vv_data, vh_data, r_data, vv_cov, vh_cov,\n",
    "                    water_thresh, flooded_thresh, not_flooded_thresh,\n",
    "                    nf_cov_thresh, f_cov_thresh, w_cov_thresh\n",
    "                   ):\n",
    "    '''Makes and returns a refined classification product based on the typical\n",
    "    inundation state and VV coefficient of variation.\n",
    "    \n",
    "    Parameters:\n",
    "    - vv_data, vh_data, r_data: VV, VH, and VV/VH arrays\n",
    "    - vv_cov, vh_cov: VV and VH coefficient of variance (cov) distributions\n",
    "    - water_thresh: ClassThreshold for the open water class\n",
    "    - flooded_thresh: ClassThreshold for the inundated class\n",
    "    - not_flooded_thresh: ClassThreshold for the not inundated class\n",
    "    - nf_cov_thresh: cov threshold for temporal change in not inundated areas\n",
    "    - f_cov_thresh: cov threshold for temporal change in inundated areas\n",
    "    - w_cov_thresh: cov threshold for temporal change in open water areas\n",
    "    '''\n",
    "    classified_image = copy.copy(r_data)\n",
    "    for row in range(classified_image.shape[0]):\n",
    "        for col in range(classified_image.shape[1]):\n",
    "            vv_val = float(vv_data[row][col])\n",
    "            vh_val = float(vh_data[row][col])\n",
    "            r_val = float(r_data[row][col])\n",
    "            water = water_thresh.is_member(vv_val, vh_val, r_val)\n",
    "            flooded = flooded_thresh.is_member(vv_val, vh_val, r_val)\n",
    "            not_flooded = not_flooded_thresh.is_member(vv_val, vh_val, r_val)\n",
    "            if water:\n",
    "                if vv_cov[row][col] > w_cov_thresh:\n",
    "                    classified_image[row][col] = 1\n",
    "                else:\n",
    "                    classified_image[row][col] = 0\n",
    "            elif flooded:\n",
    "                if vv_cov[row][col] > f_cov_thresh:\n",
    "                    classified_image[row][col] = 4\n",
    "                else:\n",
    "                    classified_image[row][col] = 3\n",
    "            else:\n",
    "                if vv_cov[row][col] > nf_cov_thresh:\n",
    "                    classified_image[row][col] = 4\n",
    "                else:\n",
    "                    classified_image[row][col] = 2\n",
    "    return(classified_image)\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "def refined_classify2(vv_data, vh_data, r_data, vv_cov, vh_cov,\n",
    "                    water_thresh, flooded_thresh, not_flooded_thresh,\n",
    "                    nf_vv_cov_thresh, f_vv_cov_thresh, w_vv_cov_thresh,\n",
    "                    nf_vh_cov_thresh, f_vh_cov_thresh, w_vh_cov_thresh\n",
    "                   ):\n",
    "    '''Makes and returns a refined classification product based on the typical\n",
    "    inundation state and VV & VH coefficients of variation.\n",
    "    \n",
    "    Parameters:\n",
    "    - vv_data, vh_data, r_data: VV, VH, and VV/VH arrays\n",
    "    - vv_cov, vh_cov: VV and VH coefficient of variance (cov) distributions\n",
    "    - water_thresh: ClassThreshold for the open water class\n",
    "    - flooded_thresh: ClassThreshold for the inundated class\n",
    "    - not_flooded_thresh: ClassThreshold for the not inundated class\n",
    "    - nf_cov_thresh: cov threshold for temporal change in not inundated areas\n",
    "    - f_cov_thresh: cov threshold for temporal change in inundated areas\n",
    "    - w_cov_thresh: cov threshold for temporal change in open water areas\n",
    "    '''\n",
    "    classified_image = copy.copy(r_data)\n",
    "    for row in range(classified_image.shape[0]):\n",
    "        for col in range(classified_image.shape[1]):\n",
    "            vv_val = float(vv_data[row][col])\n",
    "            vh_val = float(vh_data[row][col])\n",
    "            r_val = float(r_data[row][col])\n",
    "            water = water_thresh.is_member(vv_val, vh_val, r_val)\n",
    "            flooded = flooded_thresh.is_member(vv_val, vh_val, r_val)\n",
    "            not_flooded = not_flooded_thresh.is_member(vv_val, vh_val, r_val)\n",
    "            if water:\n",
    "                if vv_cov[row][col] > w_vv_cov_thresh:\n",
    "                    classified_image[row][col] = 1\n",
    "                else:\n",
    "                    classified_image[row][col] = 0\n",
    "            elif flooded:\n",
    "                if vv_cov[row][col] > f_vv_cov_thresh or vh_cov[row][col] > f_vh_cov_thresh:\n",
    "                    classified_image[row][col] = 4\n",
    "                else:\n",
    "                    classified_image[row][col] = 3\n",
    "            else:\n",
    "                if vv_cov[row][col] > nf_vv_cov_thresh or vh_cov[row][col] > nf_vh_cov_thresh:\n",
    "                    classified_image[row][col] = 4\n",
    "                else:\n",
    "                    classified_image[row][col] = 2\n",
    "    return(classified_image)\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "def refined_multilook_classify(filtered_date_sets, \n",
    "                               min_inundation, \n",
    "                               ow_min_count=False,\n",
    "                               ni_min_count=False,\n",
    "                               iv_min_count=False,\n",
    "                               return_counts=False\n",
    "                              ):\n",
    "    ''' Function to perform a refined classification integrating the\n",
    "    results of the multilooked classifications.  The final refined\n",
    "    classification is based off the count of occurences of each class\n",
    "    in each pixel across the multilooked classifications.\n",
    "    Parameters:\n",
    "    - filtered_date_sets: List of DateSets that have multilooked classifications\n",
    "    - min_inundation: Minimum inundation state classification\n",
    "    - ow_min_count: Minimum count threshold for permanent open water\n",
    "    - ni_min_count: Minimum count threshold for permanent not inundated\n",
    "    - iv_min_count: Minimum count threshold for permanent inundated vegetation\n",
    "    - return_counts: Option to return the occurence counts for each class\n",
    "    '''\n",
    "    # If minimum count thresholds have not been provided, set them to\n",
    "    # values scaled off the number of dates used\n",
    "    ndates = len(filtered_date_sets)\n",
    "    if not ow_min_count: ow_min_count = ndates/2\n",
    "    if not ni_min_count: ni_min_count = ndates\n",
    "    if not iv_min_count: iv_min_count = ndates/2\n",
    "    # Count number of occurences of each class in each pixel across\n",
    "    # classified multi-looked dates\n",
    "    nrows = np.size(min_inundation, 0)\n",
    "    ncols = np.size(min_inundation, 1)\n",
    "    ow_count = np.zeros((nrows, ncols))\n",
    "    iv_count = np.zeros((nrows, ncols))\n",
    "    ni_count = np.zeros((nrows, ncols))\n",
    "    for ds in filtered_date_sets:\n",
    "        for row in range(nrows):\n",
    "            for col in range(ncols):\n",
    "                val = ds.class_3x3[row][col]\n",
    "                if val == 1:\n",
    "                    ow_count[row][col] += 1\n",
    "                elif val == 2:\n",
    "                    ni_count[row][col] += 1\n",
    "                elif val == 3:\n",
    "                    iv_count[row][col] += 1\n",
    "    # Make refined classification based off class occurence counts\n",
    "    refined_inundation = copy.copy(min_inundation)\n",
    "    for ds in filtered_date_sets:\n",
    "        for row in range(nrows):\n",
    "            for col in range(ncols):\n",
    "                if ow_count[row][col] >= ow_min_count:\n",
    "                    refined_inundation[row][col] = 1\n",
    "                elif ni_count[row][col] >= ni_min_count:\n",
    "                    refined_inundation[row][col] = 2\n",
    "                elif iv_count[row][col] >= iv_min_count:\n",
    "                    refined_inundation[row][col] = 3\n",
    "                else:\n",
    "                    refined_inundation[row][col] = 4\n",
    "    # Return the class occurrence counts, if desired\n",
    "    if return_counts:\n",
    "        counts = [ow_count, iv_count, ni_count]\n",
    "        return([refined_inundation, counts])\n",
    "    else:\n",
    "        return(refined_inundation)\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "def prompt_already_multilooked(pn=pn):\n",
    "    '''Asks the user if they have already classified and exported the\n",
    "    multi-looked images and loads them if so.  If not, starts workflow to\n",
    "    create them.\n",
    "    \n",
    "    Parameters:\n",
    "    pn: Python version number (2 or 3)\n",
    "    '''\n",
    "    question = (\"Have you already classified and exported the \" +\n",
    "               \"multi-looked images? (y/n)\")\n",
    "    while True:\n",
    "        try:\n",
    "            if pn == 2:\n",
    "                user_in = raw_input(question)\n",
    "            elif pn == 3:\n",
    "                user_in = input(question)\n",
    "        except:\n",
    "            print(\"Invalid response, try again\")\n",
    "            continue\n",
    "        else:\n",
    "            affirmative_answers = [\"y\", \"Y\",\"yes\", \"Yes\"]\n",
    "            negative_answers = [\"n\", \"N\", \"No\", \"no\"]\n",
    "            if user_in in affirmative_answers:\n",
    "                load_classified_3x3s(filtered_date_sets, out_directory)\n",
    "                load_multilooked(filtered_date_sets, multi_directory)\n",
    "                print(\"Loaded previously classified multi-looked images.\")\n",
    "                break\n",
    "            elif user_in in negative_answers:\n",
    "                multilook_all_dates(filtered_date_sets,\n",
    "                            water_thresh, \n",
    "                            flooded_thresh,\n",
    "                            multi_directory,\n",
    "                            ulx, uly, lrx, lry, proj\n",
    "                                   )\n",
    "                print(\"Multi-looking and classification complete.\")\n",
    "                break\n",
    "            else:\n",
    "                print(\"Invalid response, try again\")\n",
    "                continue  \n",
    "        \n",
    "###############################################################################\n",
    "\n",
    "def multilook_classify(date_set, \n",
    "                       water_thresh, \n",
    "                       flooded_thresh\n",
    "                      ):\n",
    "    '''Classifies a multi-looked image and returns the result.\n",
    "    \n",
    "    Parameters:\n",
    "    - date_set: A DateSet object from date to be classified\n",
    "    - water_thresh: ClassThreshold object for open water class\n",
    "    - flooded_thresh: ClassThreshold object for inundated class\n",
    "    '''\n",
    "    vv_3x3 = moving_window(date_set.vv_df.read_data())\n",
    "    vh_3x3 = moving_window(date_set.vh_df.read_data())\n",
    "    r_3x3 = vv_3x3/vh_3x3\n",
    "    class_3x3 = triple_classify(vv_3x3, vh_3x3, r_3x3, \n",
    "                                water_thresh, \n",
    "                                flooded_thresh \n",
    "                               )\n",
    "    datef = date_set.datef\n",
    "    date_set.close()\n",
    "    return(class_3x3, datef)\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "def multilook_all_dates(filtered_date_sets,\n",
    "                        water_thresh, \n",
    "                        flooded_thresh,\n",
    "                        multi_directory,\n",
    "                        ulx, uly, lrx, lry, proj,\n",
    "                        classify=True\n",
    "                        ):\n",
    "    '''Performs multi-looking on all input dates and stores result as an \n",
    "    attribute (class_3x3) in the input DateSets.\n",
    "    \n",
    "    Parameters:\n",
    "    - filtered_date_sets: List of filtered DateSets\n",
    "    - water_thresh: ClassThreshold object for open water class\n",
    "    - flooded_thresh: ClassThreshold object for inundated class\n",
    "    - multi_directory: Directory where multilooked images are stored\n",
    "    - ulx, uly, lrx, lry: Corner coordinates for exporting\n",
    "    - proj: Projection for exporting\n",
    "    '''\n",
    "    nds = len(filtered_date_sets)\n",
    "    for idx, ds in enumerate(filtered_date_sets):\n",
    "        print(\n",
    "            str(datetime.now().time().strftime(\"%H:%M:%S\")) \n",
    "            + \") Processing data from \" \n",
    "            + ds.datef \n",
    "            + \"... [\" \n",
    "            + str(idx+1) \n",
    "            + \"/\" \n",
    "            + str(nds) \n",
    "            + \"]\"\n",
    "        )\n",
    "        vv_3x3 = moving_window(ds.vv_df.read_data())\n",
    "        vv_out = multi_directory + ds.vv_df.name.split('.')[0] + '3x3.tif'\n",
    "        proc_export(vv_3x3, ulx, uly, lrx, lry, vv_out, proj, True)\n",
    "        vh_3x3 = moving_window(ds.vh_df.read_data())\n",
    "        vh_out = multi_directory + ds.vh_df.name.split('.')[0] + '3x3.tif'\n",
    "        proc_export(vh_3x3, ulx, uly, lrx, lry, vh_out, proj, True)\n",
    "        r_3x3 = vv_3x3/vh_3x3\n",
    "        if classify:\n",
    "            ds.class_3x3 = triple_classify(\n",
    "                vv_3x3, vh_3x3, r_3x3, \n",
    "                water_thresh, flooded_thresh\n",
    "            )\n",
    "        ds.close()\n",
    "        \n",
    "###############################################################################\n",
    "\n",
    "def interactive_area_plot(classified_date_sets):\n",
    "    '''Creates an interactive time series plot showing the total area and \n",
    "    percentage of total area inundated.\n",
    "    \n",
    "    Parameters:\n",
    "    - classified_date_sets: List of DateSets with classified results\n",
    "    '''\n",
    "    # Modified code from https://plot.ly/python/range-slider/\n",
    "    dateps = []\n",
    "    areas= []\n",
    "    aps = []\n",
    "    for ds in classified_date_sets:\n",
    "        dateps.append(ds.datep)\n",
    "        [ds.area, ds.percent, total] = calculate_area_inundated(ds.class_3x3)\n",
    "        areas.append(0.0001*ds.area)\n",
    "        aps.append([0.0001*ds.area, ds.percent])\n",
    "    area_trace = go.Scatter(\n",
    "      x = dateps,\n",
    "      y = areas,\n",
    "      name = \"Inundated area\",\n",
    "      text = [str(ap[0]) + \"ha (%.2f%%)\" % ap[1] for ap in aps],\n",
    "      yaxis = \"y\"\n",
    "    )\n",
    "\n",
    "    data = [area_trace]\n",
    "\n",
    "    # style all the traces\n",
    "    for k in range(len(data)):\n",
    "        data[k].update(\n",
    "            {\n",
    "                \"hoverinfo\": \"name+x+text\",\n",
    "                \"line\": {\"width\": 0.5}, \n",
    "                \"marker\": {\"size\": 8},\n",
    "                \"mode\": \"lines+markers\",\n",
    "                \"showlegend\": False\n",
    "            }\n",
    "        )\n",
    "\n",
    "    layout = {\n",
    "      \"dragmode\": \"zoom\", \n",
    "      \"hovermode\": \"x\", \n",
    "      \"legend\": {\"traceorder\": \"reversed\"}, \n",
    "      \"margin\": {\n",
    "        \"t\": 100, \n",
    "        \"b\": 100\n",
    "      }, \n",
    "      \"xaxis\": {\n",
    "        \"autorange\": True, \n",
    "        \"range\": [\"2017-10-31 18:36:37.3129\", \"2018-05-10 05:23:22.6871\"], \n",
    "        \"rangeslider\": {\n",
    "          \"autorange\": True, \n",
    "          \"range\": [\"2017-10-31 18:36:37.3129\", \"2018-05-10 05:23:22.6871\"]\n",
    "        }, \n",
    "        \"type\": \"date\"\n",
    "      }, \n",
    "      \"yaxis\": {\n",
    "        \"anchor\": \"x\", \n",
    "        \"autorange\": True, \n",
    "        \"domain\": [0, 1], \n",
    "        \"linecolor\": \"#673ab7\", \n",
    "        \"mirror\": True, \n",
    "        \"range\": [0,10000], \n",
    "        \"showline\": True, \n",
    "        \"side\": \"right\", \n",
    "        \"tickfont\": {\"color\": \"#673ab7\"}, \n",
    "        \"tickmode\": \"auto\", \n",
    "        \"ticks\": \"\",\n",
    "        \"title\": \"Inundated area (ha)\",\n",
    "        \"titlefont\": {\"color\": \"#673ab7\"}, \n",
    "        \"type\": \"linear\", \n",
    "        \"zeroline\": False\n",
    "        }\n",
    "    }\n",
    "    plotly.offline.init_notebook_mode(connected=True)\n",
    "    fig = go.Figure(data=data, layout=layout)\n",
    "    plotly.offline.iplot(fig)\n",
    "        \n",
    "###############################################################################\n",
    "        \n",
    "def min_max_inundation(typical_inundation, classified_date_sets):\n",
    "    '''Returns the minimum and maximum inundation state by comparing all\n",
    "    classified dates.\n",
    "    \n",
    "    Parameters:\n",
    "    - typical_inundation: Classification product from the multi-temporal \n",
    "        averages\n",
    "    - classified_date_sets: DateSets that contain classifications for the \n",
    "        multi-looked products\n",
    "    '''\n",
    "    max_inundation = copy.copy(typical_inundation)\n",
    "    min_inundation = copy.copy(typical_inundation)\n",
    "    for row in range(typical_inundation.shape[0]):\n",
    "        for col in range(typical_inundation.shape[1]):\n",
    "            pixel_class_values = [\n",
    "                int(ds.class_3x3[row][col]) for ds in classified_date_sets\n",
    "            ]\n",
    "            default_class = typical_inundation[row][col]\n",
    "            if 3 in pixel_class_values: max_inundation[row][col] = 3\n",
    "            \n",
    "            if 2 in pixel_class_values: min_inundation[row][col] = 2\n",
    "            elif 1 in pixel_class_values: min_inundation[row][col] = 1\n",
    "            else: min_inundation[row][col] = 3                \n",
    "    return(min_inundation, max_inundation) \n",
    "\n",
    "###############################################################################\n",
    "        \n",
    "def classified_plot(classified_image, datef=False):\n",
    "    '''Plots a classified product using 5 classes.\n",
    "    \n",
    "    Parameters:\n",
    "    - classified_image: 2D array of classified values\n",
    "    - datef (optional): Formatted date string to be plotted along y-axis\n",
    "    '''\n",
    "    fig, ax = plt.subplots(figsize=(9.5,9.5))\n",
    "    colors = ['b', 'c', 'g', 'y', 'm']\n",
    "    cmap = matplotlib.colors.ListedColormap(colors)\n",
    "    cax = ax.imshow(classified_image, cmap=cmap, vmin = -.2, vmax = 4.2)\n",
    "    cbar = fig.colorbar(cax, ticks=[0,1,2,3,4])\n",
    "    cbar.ax.set_yticklabels(\n",
    "        ['Permanent Open Water', 'Seasonal Open Water', \n",
    "         'Permanent Not Inundated', 'Permanent Inundated Vegetation', \n",
    "         'Seasonal Inundation']\n",
    "    )\n",
    "    title_str = \"Classified using VV, VH, and VV/VH brightness values: \"\n",
    "    if datef: title_str += datef\n",
    "    else: title_str += \"Multi-temporal average\"\n",
    "    ax.set_title(title_str)\n",
    "    \n",
    "###############################################################################\n",
    "    \n",
    "def classified_plot2(classified_image, datef=False, colors=['b','g','y']):\n",
    "    '''Plots classified product with 3 classes (Open water, not inundated,\n",
    "    and inundated).\n",
    "    \n",
    "    Parameters:\n",
    "    - classified_image: 2D array of classified values\n",
    "    - datef (optional): Formatted date string to be plotted along y-axis\n",
    "    - colors: List containing the names of the colors for the 3 classes\n",
    "    '''\n",
    "    fig, ax = plt.subplots(figsize=(9.5,9.5))\n",
    "    cax = ax.imshow(\n",
    "        classified_image, \n",
    "        cmap=matplotlib.colors.ListedColormap(colors), \n",
    "        vmin = 0, vmax = 4\n",
    "    )\n",
    "    cbar = fig.colorbar(cax, ticks=[.67,2,3.33])\n",
    "    cbar.ax.set_yticklabels(['Open Water', 'Not Inundated', 'Inundated'])\n",
    "    title_str = \"Classified using VV, VH, and VV/VH brightness values: \"\n",
    "    if datef: title_str += datef\n",
    "    else: title_str += \"Multi-temporal average\"\n",
    "    ax.set_title(title_str)\n",
    "    \n",
    "###############################################################################\n",
    "    \n",
    "def refined_plot(classified_image, datef=False, colors=['b', 'w', 'y', 'g']):\n",
    "    '''Plots the results of the refined classification (4 classes).\n",
    "    \n",
    "    Parameters:\n",
    "    - classified_image: 2D array of refined classification values\n",
    "    - datef (optional): Formatted date string to be plotted along y-axis\n",
    "    '''\n",
    "    fig, ax = plt.subplots(figsize=(9.5,9.5))\n",
    "    cax = ax.imshow(\n",
    "        classified_image, \n",
    "        cmap=matplotlib.colors.ListedColormap(colors), \n",
    "        vmin = .5, vmax = 4.5\n",
    "    )\n",
    "    cbar = fig.colorbar(cax, ticks=[1,2,3,4])\n",
    "    cbar.ax.set_yticklabels(\n",
    "        ['Permanent open water', \n",
    "         'Not inundated', \n",
    "         'Permanent inundated vegetation', 'Seasonal inundation'\n",
    "        ]\n",
    "    )\n",
    "    title_str = \"Refined classification using VV, VH, and VV/VH values: \"\n",
    "    if datef: title_str += datef\n",
    "    else: title_str += \"Multi-temporal average\"\n",
    "    ax.set_title(title_str)    \n",
    "    \n",
    "###############################################################################\n",
    "    \n",
    "def extract_spatial_metadata(data_file):\n",
    "    '''Extracts and returns the projection, upper left, and lower right \n",
    "    corners' projected coordinates to use when exporting.\n",
    "    \n",
    "    Parameters:\n",
    "    - data_file: DataFile\n",
    "    '''\n",
    "    data_file.read_data()\n",
    "    prof = data_file.raw.profile\n",
    "    affine = prof['transform']\n",
    "    ulx, uly = affine[2], affine[5]\n",
    "    height = prof['height']\n",
    "    width = prof['width']\n",
    "    dx = affine[0]\n",
    "    dy = affine[4]\n",
    "    lrx = ulx + width*dx\n",
    "    lry = uly + height*dy\n",
    "    proj = \"EPSG:\" + str(prof['crs']).split('EPSG:')[-1]\n",
    "    data_file.close()\n",
    "    return(ulx, uly, lrx, lry, proj)\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "def proc_export(array, \n",
    "                ulx, uly, lrx, lry, \n",
    "                outpath, \n",
    "                proj=\"EPSG:32605\", \n",
    "                clean_temp=True\n",
    "               ):\n",
    "    '''Exports array as a raster file (.tif).\n",
    "    \n",
    "    Parameters: \n",
    "    - array: 2D image array\n",
    "    - ulx, uly: Projected x and y coordinates of upper left corner\n",
    "    - lrx, lry: Projected x and y coordinates of lower right corner\n",
    "    - outpath: Path for output file\n",
    "    - proj: Projection in GDAL-readable format\n",
    "    '''\n",
    "    if not outpath.name.endswith('tif'):\n",
    "        print(\"Output filename must end with .tif\")\n",
    "        return(False)\n",
    "    else:\n",
    "        # Create temporary TIF file\n",
    "        temp_path = str(outpath).split(\".tif\")[0] + \"_temp.tif\"\n",
    "        temp_image = Image.fromarray(array)\n",
    "        temp_image.save(temp_path)\n",
    "        \n",
    "        # Add spatial information with GDAL\n",
    "        translate_command = \"gdal_translate\"\n",
    "        translate_command += \" -a_srs \" + proj\n",
    "        translate_command += \" -a_ullr \" + str(ulx) + \" \" + str(uly)\n",
    "        translate_command += \" \" + str(lrx) + \" \" + str(lry)\n",
    "        translate_command += \" \" + temp_path + \" \" + str(outpath)\n",
    "        subprocess.call(translate_command, shell=True)\n",
    "        \n",
    "        # Clean temporary file\n",
    "        if clean_temp:\n",
    "            clean_command = \"rm \" + temp_path\n",
    "            subprocess.call(clean_command, shell=True)\n",
    "###############################################################################\n",
    "\n",
    "def export_classified_3x3(classified_date_sets, out_dir):\n",
    "    '''Exports the results of the classification of the multi-looked images as \n",
    "    .tif files.\n",
    "    \n",
    "    Parameters:\n",
    "    - classified_date_sets: List of DateSets with multi-look classifications\n",
    "    - out_dir: Output directory\n",
    "    '''\n",
    "    for ds in classified_date_sets:\n",
    "        c3x3 = ds.class_3x3\n",
    "        outpath = out_dir/(ds.date + '_inundation.tif')\n",
    "        proc_export(c3x3, ulx, uly, lrx, lry, outpath, proj, True)\n",
    "###############################################################################\n",
    "\n",
    "def load_multilooked(filtered_dates_sets, multi_directory):\n",
    "    for ds in filtered_date_sets:\n",
    "        vv_out = multi_directory/(ds.vv_df.name.split('.')[0] + '3x3.tif')\n",
    "        vv_raw = rasterio.open(vv_out)\n",
    "        \n",
    "        ds.vv_3x3 = vv_raw.read(\n",
    "            1, out_shape=(1, int(vv_raw.height), int(vv_raw.width))\n",
    "        )\n",
    "        vh_out = multi_directory/(ds.vh_df.name.split('.')[0] + '3x3.tif')\n",
    "        vh_raw = rasterio.open(vh_out)\n",
    "        \n",
    "        ds.vh_3x3 = vh_raw.read(\n",
    "        1, out_shape=(1, int(vh_raw.height), int(vh_raw.width))\n",
    "        )\n",
    "        ds.r_3x3 = ds.vv_3x3/ds.vh_3x3\n",
    "\n",
    "        \n",
    "###############################################################################\n",
    "\n",
    "def load_classified_3x3s(filtered_dates_set, out_dir):\n",
    "    '''Loads the previously exported multi-looked classifications and appends\n",
    "    them to existing DateSets.\n",
    "    \n",
    "    Parameters:\n",
    "    - filtered_date_sets: DateSets that have been filtered for inclusion\n",
    "    - out_dir: Path to directory where previously exported results are stored\n",
    "    '''\n",
    "    for ds in filtered_date_sets:\n",
    "        data_dir_search = \"*\" + ds.date + \"_inundation.tif\"\n",
    "        search = list(out_dir.rglob(data_dir_search))\n",
    "        if len(search) == 1:\n",
    "            raw = rasterio.open(search[0])\n",
    "            ds.class_3x3 = raw.read(\n",
    "                1, out_shape=(1, int(raw.height), int(raw.width))\n",
    "            )       \n",
    "###############################################################################\n",
    "\n",
    "def calculate_area_inundated(classified, res=10):\n",
    "    '''Calculates and returns area inundated (and percent area inundated).\n",
    "    \n",
    "    Parameters:\n",
    "    - classified: Classified 2D array\n",
    "    - res: Pixel size (in projected units, e.g. meters)\n",
    "    '''\n",
    "    nrow, ncol = classified.shape\n",
    "    xlength = ncol*res\n",
    "    ylength = nrow*res\n",
    "    total_area = xlength * ylength\n",
    "    count_inundated = 0\n",
    "    res2 = res**2\n",
    "    for row in range(nrow):\n",
    "        for col in range(ncol):\n",
    "            if classified[row][col] == 3: count_inundated += 1\n",
    "    area_inundated = count_inundated*res2\n",
    "    percent_inundated = float(area_inundated)*100/total_area\n",
    "    return([area_inundated, percent_inundated, total_area])       \n",
    "\n",
    "###############################################################################\n",
    "            \n",
    "def proc_resample(path, overwrite=False):\n",
    "    '''Resamples a file using the GDAL shell library and creates a new file.\n",
    "    \n",
    "    Parameters:\n",
    "    - path: Path to the file to be resampled\n",
    "    - overwrite: Option to overwrite existing resampled result\n",
    "    '''\n",
    "    output = path.split('.tif')[0] + '_resampled.tif'\n",
    "    pieces = [\"gdalwarp -tr 100 100\", path, output]\n",
    "    if overwrite: pieces.append(\"-overwrite\")\n",
    "    gdalwarp_command = \" \".join(pieces)\n",
    "    print(gdalwarp_command)\n",
    "    subprocess.call(gdalwarp_command, shell=True)\n",
    "\n",
    "def resample_all(directory, overwrite=False):\n",
    "    '''Resamples all files in a specified directory.\n",
    "    \n",
    "    Parameters:\n",
    "    - directory: Path to directory\n",
    "    - overwrite: Option to overwrite existing resampled output\n",
    "    '''\n",
    "#     figs = os.listdir(directory)\n",
    "    figs = directory.iterdir()\n",
    "    paths = [directory/fig for fig in figs] \n",
    "#     paths = [os.path.join(directory, fig) for fig in figs] \n",
    "    for path in paths:\n",
    "        if 'resampled' not in str(path) and str(path).endswith('.tif'):\n",
    "            proc_resample(path, overwrite) \n",
    "            \n",
    "###############################################################################\n",
    "        \n",
    "def proc_gdalwarp(infile,  proj=\"EPSG:32605\", outfile=\"null\"):\n",
    "    '''Projects a raster into specified projection.\n",
    "    \n",
    "    Parameters:\n",
    "    - infile: Path to file to be projected\n",
    "    - proj: String in GDAL-readable format specifying projection (default is\n",
    "        WGS 84 - UTM Zone 5N).\n",
    "    - outfile: Path to output file (filename will be automatically generated\n",
    "        if none is specified)\n",
    "    '''\n",
    "    if outfile == \"null\":\n",
    "        ext = infile.split(\".\")[-1]\n",
    "        outfile = infile.split(\".\")[0] + \"_proj.\" + ext\n",
    "    warp_command = \"gdalwarp -t_srs \" + proj + \" \" + infile + \" \" + outfile\n",
    "    subprocess.call(warp_command, shell=True)\n",
    "    \n",
    "###############################################################################\n",
    "\n",
    "def proc_translate(infile, outfile=\"null\"):\n",
    "    '''Converts a raster file into ENVI format and produces an ENVI header \n",
    "    file while doing so.\n",
    "    \n",
    "    Parameters:\n",
    "    - infile: Path to file to be converted\n",
    "    - outfile: Path to output file\n",
    "    '''\n",
    "    if outfile == \"null\":\n",
    "        outfile = infile.split(\".\")[0] + \".img\"\n",
    "    translate_command = \"gdal_translate -of ENVI \" + infile + \" \" + outfile\n",
    "    subprocess.call(translate_command, shell=True)\n",
    "    \n",
    "###############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### **Load Data**\n",
    "\n",
    "For this exercise, we will be using a VV/VH Sentinel-1 data stack over Selawik, Alaska. The town of Selawik is located in the northwest of Alaska on the coast to the Chukchi and Bering sea. It is prone to heavy rains and extensive inundation during the breakup and summer seasons. We will use Sentinel-1 data to map inundated vegetation and understand the persistence of inundation in the area.\n",
    "    \n",
    "Before we get started, let's first **create a working directory for this analysis and download the relevant data into the notebook**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_dir = Path(\"/home/jovyan/notebooks/SAR_Training/English/Ecosystems/S1-InundationMapping/\")\n",
    "\n",
    "if not project_dir.exists():\n",
    "    project_dir.mkdir()\n",
    "\n",
    "time_series_path = 's3://asf-jupyter-data-west/S1-InundationMapping.zip'\n",
    "time_series = Path(time_series_path).name\n",
    "!aws --region=us-west-2 --no-sign-request s3 cp $time_series_path $time_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Path(time_series).exists():\n",
    "    asfn.asf_unzip(str(project_dir), time_series)\n",
    "    Path(time_series).unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these directories are within S1-InundationMapping dir that you just made\n",
    "data_directory = project_dir/'S1-InundationMapping'/'data'/'SelawikZoom'\n",
    "out_directory = project_dir/'S1-InundationMapping'/'out'/'SelawikZoom'\n",
    "multi_directory = data_directory/'multi/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### **Selection Process**\n",
    "\n",
    "After selecting the desired time range, this section outputs a list of the files, their VV, VH, and VV/VH ratio value ranges, image thumbnails, and plots of image brightness.  High increases in image brightness may be due to different reasons such as the presence of snow or ice cover, wind and weather changes, or calibration errors, but with the selection options below, users can visually evaluate the image collection and select which dates to include or exclude from analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of files in data_directory with the correct file extension\n",
    "# and contain the search string in their filename\n",
    "ext = \".tif\"\n",
    "search = \"subset\"\n",
    "data_files = get_files(data_directory, ext, search, \"new\")\n",
    "\n",
    "# Find dates\n",
    "dates = find_dates(data_files)\n",
    "\n",
    "# Make DateSets (store VV and VH DataFiles into a single object)\n",
    "date_sets = make_datesets(data_files, dates)\n",
    "datesf = [ds.datef for ds in date_sets]\n",
    "\n",
    "# Find minimum and maximum VV and VH values of the DataFiles (use these to \n",
    "# standardize color limits)\n",
    "[[vv_min, vv_max], [vh_min, vh_max], [r_min, r_max]] = find_minmax(date_sets)\n",
    "\n",
    "# Print dates\n",
    "print(\"\\nDates:\")\n",
    "datesf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Make adjustments to plotting color limits\n",
    "vv_min = 0 #minimum VV value\n",
    "vv_max = .15 #maximum VV value\n",
    "vh_min = 0#minimum VH value\n",
    "vh_max = .03 #maximum VH value\n",
    "r_min = 0 #minimum VV/VH value\n",
    "r_max = 15 #maximum VV/VH value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Plot all images with checkboxes to select images for exclusion:\n",
    "ws3 = multi_checkbox_widget_dateimages(\n",
    "    date_sets, \n",
    "    vv_min, vv_max, \n",
    "    vh_min, vh_max,\n",
    "    r_min, r_max\n",
    ")\n",
    "ws3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get indices of checked options\n",
    "#(Can only be run once per creation of the box above, need to re-run code above)\n",
    "excluded_dates3 = get_excluded_dates3(ws3, date_sets)\n",
    "print(\"Dates to be excluded:\")\n",
    "excluded_dates3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Interactive plot time series of average brightness per image\n",
    "interactive_backscatter_plot(date_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make time slider to select start and end dates\n",
    "selection_range_slider = make_timeslider(date_sets)\n",
    "selection_range_slider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get start and end dates from time slider (re-run this to get updated times)\n",
    "slider_min, slider_max = get_slider_vals(selection_range_slider)\n",
    "# Filter dates according to checkbox and time slider\n",
    "filtered_date_sets = filter_date_sets(date_sets, excluded_dates3, slider_min, slider_max)\n",
    "\n",
    "# Extract spatial metdata from filtered images to export classified products\n",
    "ulx, uly, lrx, lry, proj = extract_spatial_metadata(filtered_date_sets[0].vv_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### **Classification Overview**\n",
    "\n",
    "Typical inundation is defined as the classification of the multi-temporal average iamges (Ia).\n",
    "\n",
    "**Classify Inundation:**\n",
    "\n",
    "- Calculate multi-temporal average for VV, VH, and VV/VH\n",
    "- Derive typical classification from multi-temporal averages with rules-based classification\n",
    "- For each date:\n",
    "    - Calculate multi-looked view for VV, VH, and VV/VH\n",
    "    - Perform initial classification\n",
    "    - Compare to corresponding multi-temporal average to determine changes\n",
    "    - Make refined classification based on change with rules\n",
    "- For each pixel:\n",
    "    - Loop over all refined classifications to find minimum and maximum inundation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **Multi-Temporal Averages**\n",
    "\n",
    "This section produces a multi-temporal radar backscatter image (Ia) by averaging the data from each acquisition of VV, VH, and VV/VH over the selected date ranges.\n",
    "\n",
    "In general, SAR data can be noisy and speckled.  However, averaging images reduces speckle and smooths the imagery\n",
    "and can be used to examine changes over time.  In areas where land cover and terrain remain the same, the level of\n",
    "speckle visible in individual scenes is reduced, while dynamic areas of environmental change will reflect\n",
    "variations in backscatter.  This can be particularly evident in areas of inundation, as open water and inundated\n",
    "vegetation are at opposite ends of the range of radar backscatter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate multi-temporal average for VV, VH, and VV/VH over selected date range\n",
    "(vv_avg, vh_avg, r_avg, vv_cov, vh_cov) = calculate_multitemporal_avg(filtered_date_sets)\n",
    "# Plot them:\n",
    "fig_xsize = 10\n",
    "fig_ysize = 7\n",
    "f = vv_vh_r_plot(vv_avg, vh_avg, r_avg, \n",
    "                 vv_min, vv_max, \n",
    "                 vh_min, vh_max, \n",
    "                 r_min, r_max, \n",
    "                 \"Multi-temporal averages\", \n",
    "                 fig_xsize, fig_ysize\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Large plot of multi-temporal average of VV values to inspect pixel values\n",
    "fig_xsize = 8\n",
    "fig_ysize = 8\n",
    "big_fig(fig_xsize, fig_ysize)\n",
    "gray_plot(vv_avg, vv_min, vv_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Full-resolution plot of multi-temporal average of VV \n",
    "dpi = 192 #change this to match your monitor's DPI\n",
    "cursor = True #option to enable data cursor in resulting image\n",
    "full = True #option to increase Jupyter Notebook width to monitor size \n",
    "# pixel2pixel_plot(vv_avg, vv_min, vv_max, dpi, cursor, full) # Comment/uncomment this line to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Large plot of multi-temporal average of VH values to inspect pixel values\n",
    "big_fig(fig_xsize, fig_ysize)\n",
    "gray_plot(vh_avg, vh_min, vh_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full-resolution plot of multi-temporal average of VH\n",
    "dpi = 192 #change this to match your monitor's DPI\n",
    "cursor = True #option to enable data cursor in resulting image\n",
    "full = True #option to increase Jupyter Notebook width to monitor size \n",
    "# pixel2pixel_plot(vh_avg, vh_min, vh_max, dpi, cursor, full) # Comment/uncomment this line to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Large plot of multi-temporal average of VV/VH values to inspect pixel values\n",
    "big_fig(fig_xsize, fig_ysize)\n",
    "gray_plot(r_avg, r_min, r_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full-resolution plot of multi-temporal average of VV/VH\n",
    "dpi = 192 #change this to match your monitor's DPI\n",
    "cursor = True #option to enable data cursor in resulting image\n",
    "full = True #option to increase Jupyter Notebook width to monitor size \n",
    "# pixel2pixel_plot(r_avg, r_min, r_max, dpi, cursor, full) # uncomment this line to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot coefficient of variation for VV and VH\n",
    "f = vv_vh_cov_plot(vv_cov, vh_cov,\n",
    "                 0, 1, \n",
    "                 0, 1, \n",
    "                 \"Coefficient of Variation\", \n",
    "                 10, 7\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Large plot of VV coefficient of variation\n",
    "big_fig(fig_xsize, fig_ysize)\n",
    "gray_plot(vv_cov, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full resolution plot of VV/VH coefficient of variation\n",
    "dpi = 192 #change this to match your monitor's DPI\n",
    "cursor = True #option to enable data cursor in resulting image\n",
    "full = True #option to increase Jupyter Notebook width to monitor size \n",
    "# pixel2pixel_plot(r_cov, 0, 1, dpi, cursor, full) # Comment/uncomment this line to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate VV/VH coefficient of variation\n",
    "r_cov = calculate_r_cov(filtered_date_sets)\n",
    "# Large plot of VV/VH coefficient of variation\n",
    "big_fig(fig_xsize, fig_ysize)\n",
    "gray_plot(r_cov, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full resolution plot of VV/VH coefficient of variation\n",
    "dpi = 192 #change this to match your monitor's DPI\n",
    "cursor = True #option to enable data cursor in resulting image\n",
    "full = True #option to increase Jupyter Notebook width to monitor size \n",
    "# pixel2pixel_plot(r_cov, 0, 1, dpi, cursor, full) # Comment/uncomment this line to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export multi-temporal averages and coefficient of variation\n",
    "# VV\n",
    "vv_avg_out = out_directory/'vv_avg.tif'\n",
    "proc_export(vv_avg, ulx, uly, lrx, lry, vv_avg_out, proj, True)\n",
    "# VH\n",
    "vh_avg_out = out_directory/'vh_avg.tif'\n",
    "proc_export(vh_avg, ulx, uly, lrx, lry, vh_avg_out, proj, True)\n",
    "# VV/VH\n",
    "r_avg_out = out_directory/'r_avg.tif'\n",
    "proc_export(r_avg, ulx, uly, lrx, lry, r_avg_out, proj, True)\n",
    "# VV CoV\n",
    "vv_cov_out = out_directory/'vv_cov.tif'\n",
    "proc_export(vv_cov, ulx, uly, lrx, lry, vv_cov_out, proj, True)\n",
    "# VH CoV\n",
    "vh_cov_out = out_directory/'vh_cov.tif'\n",
    "proc_export(vh_cov, ulx, uly, lrx, lry, vh_cov_out, proj, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "\n",
    "# Derive typical inundation from multi-temporal averages with rules-based \n",
    "# classification. These thresholds are determined through pixel inspection \n",
    "# and analysis from temporal averages, and can be adjusted by the user. \n",
    "\n",
    "# Set threshold values for the different classes\n",
    "'''Example:\n",
    "class_thresh = ClassThresholds(class_vv_min, class_vv_max,\n",
    "                                class_vh_min, class_vh_max,\n",
    "                                class_r_min, class_r_max\n",
    "                                )\n",
    "'''\n",
    "water_thresh = ClassThresholds(0, 0.07,\n",
    "                               0, 0.0045,\n",
    "                               0, 1000\n",
    "                              )\n",
    "\n",
    "flooded_thresh = ClassThresholds(0.12, 1,\n",
    "                                 .0045, 1.5,\n",
    "                                 4, 50\n",
    "                                )\n",
    "\n",
    "# Typical inundation, or the average inundation over a time period, is \n",
    "# found using a triple classification function. In this triple classification,\n",
    "# the function runs through and uses each of the different class thresholds\n",
    "# from VV, VH,and VV/VH to create a single classification output. \n",
    "\n",
    "# Find typical inundation from multi-temporal averages of VV, VH, and VV/VH\n",
    "typical_inundation = triple_classify(\n",
    "    vv_avg, vh_avg, r_avg, \n",
    "    water_thresh, flooded_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot typical inundation state\n",
    "classified_plot2(typical_inundation)\n",
    "#plt.savefig('typical_inundation.pdf') #Save figure as pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export typical inundation state as GeoTIFF\n",
    "typical_out = out_directory/'typical_inundation.tif'\n",
    "proc_export(typical_inundation, ulx, uly, lrx, lry, typical_out, proj, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Multi-looked Classification**\n",
    "\n",
    "Inundation extent can fluctuate greatly.  Individual images or dates can also be classified to analyze temporal variability and make specific date comparisons.  Multi-looked views are applied to each date's VV, VH, and VV/VH images to help smooth and reduce speckle.  The multi-look view uses a 3x3 moving window to average neighboring pixels. \n",
    "\n",
    "**For each date:** \n",
    "\n",
    "- Calculate multi-looked view for VV, VH, and VV/VH\n",
    "- Perform rules-based classification\n",
    "    \n",
    "Note: I precalculated all necessary multilooked images. <font color='rgba(200,0,0,0.2)'><b>So please answer the question in the next code cell with YES!</b></font>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask user if they have previously classified and exported the multi-looked images.\n",
    "# If the user has not classified and exported the multi-looked images, \n",
    "    # the script will multi-look all filtered dates using a 3x3 moving window, \n",
    "    # perform rules-based classification,\n",
    "    # and save the classification to the DateSet objects in filtered_date_sets.\n",
    "    # WARNING: This takes several minutes per date!\n",
    "# If the user has already classified and exported the multi-looked images, \n",
    "    # the script will load the exported .tif files located in ./out/\n",
    "    # and append them to the DateSet objects in filtered_date_sets.\n",
    "    \n",
    "# NOTE: This step takes very long!! Therefore, I have preprocessed all multi-looked data for you. \n",
    "# PLEASE ANSWER THE PROMPTED QUESTION WITH YES for this data set!!\n",
    "prompt_already_multilooked()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export all classified multi-looked dates\n",
    "export_classified_3x3(filtered_date_sets, out_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and plot change in inundated area over time\n",
    "interactive_area_plot(filtered_date_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find minimum and maximum inundation\n",
    "min_inundation, max_inundation = min_max_inundation(\n",
    "    typical_inundation, \n",
    "    filtered_date_sets\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot minimum inundation\n",
    "classified_plot2(min_inundation, \"Minimum inundation\")\n",
    "#plt.savefig('minimum_inundation.pdf') #Save figure as pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot maximum inundation\n",
    "classified_plot2(max_inundation, \"Maximum inundation\")\n",
    "#plt.savefig('maximum_inundation.pdf') #Save figure as pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export minimum and maximum inundation maps as GeoTIFF\n",
    "min_out = out_directory/'minimum_inundation.tif'\n",
    "max_out = out_directory/'maximum_inundation.tif'\n",
    "proc_export(min_inundation, ulx, uly, lrx, lry, min_out, proj, True)\n",
    "proc_export(max_inundation, ulx, uly, lrx, lry, max_out, proj, True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Multi-looked-based refined classification**\n",
    "Detect areas of seasonal inundation using change detected from multi-looked dates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform refined classification based on number of class occurrences per pixel\n",
    "# across all multi-looked dates.  User can elect to alter the default minimum\n",
    "# counts below by changing the min_count values to a number less than \n",
    "# or equal to the number of multi-looked dates.\n",
    "\n",
    "# Class occurrence count thresholds\n",
    "# (Leave as False to use default values, or change to numeric values)\n",
    "ow_min_count=False #Open water minimum count\n",
    "ni_min_count=10 #Not inundated minimum count\n",
    "iv_min_count=10 #Inundated vegetation minimum count\n",
    "\n",
    "refined_inundation = refined_multilook_classify(filtered_date_sets, min_inundation,\n",
    "                                               ow_min_count, ni_min_count, iv_min_count\n",
    "                                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot refined classification\n",
    "refined_plot(refined_inundation, \"Refined classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export refined classification as GeoTIFF\n",
    "refined_out = out_directory/\"refined_classification2_dswe.tif\"\n",
    "proc_export(refined_inundation, ulx, uly, lrx, lry, refined_out, proj, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## **8. Conclusion**\n",
    "\n",
    "Multi-temporal SAR data data from Sentinel-1 are a good basis for identifying inundated areas and distinguish seasonal inundation from short term inundation. Note, however, that due to limited penetration into dense vegetation, the performance of C-band Sentinel-1 data for inundation mapping in Colombia will be limited. A better choice will be L-band SAR data from future missions such as NISAR. The higher penetration of L-band will improve inundation mapping performance. The same workflow can be used for these future L-band data. \n",
    "\n",
    "For a bit more information on change detection and SAR in general, please look at the recently published [SAR Handbook: Comprehensive Methodologies for Forest Monitoring and Biomass Estimation](https://gis1.servirglobal.net/TrainingMaterials/SAR/SARHB_FullRes.pdf).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "*Exercise7-InundationMappingfromSARTimeSeries-Example.ipynb - Version 1.4.1 - November 2021*\n",
    "\n",
    "**Version Changes:**\n",
    "\n",
    "- Replaced `os` module with `pathllib` counterparts\n",
    "- Certain functions are modified significantly to accomodate `pathlib`\n",
    "- Converted `html` to `Markdown`\n",
    "- Replaced JavaScript cell with `url_widget`\n",
    "- `asfn_notebook` replaced with `opensarlab_lib`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rtc_analysis",
   "language": "python",
   "name": "conda-env-.local-rtc_analysis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
